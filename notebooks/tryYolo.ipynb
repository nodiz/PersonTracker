{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import time\n",
    "import datetime\n",
    "import argparse\n",
    "import random\n",
    "import os, sys\n",
    "\n",
    "\n",
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import datasets\n",
    "from torch.autograd import Variable\n",
    "\n",
    "import torchvision.transforms as T\n",
    "from torch.utils.data import Dataset\n",
    "from torch.utils.data import DataLoader\n",
    "from matplotlib.ticker import NullLocator\n",
    "\n",
    "from PIL import Image\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.patches as patches\n",
    "\n",
    "from matplotlib.colors import ListedColormap, LinearSegmentedColormap\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "from tqdm import tqdm\n",
    "\n",
    "import io\n",
    "import PIL\n",
    "import requests\n",
    "import cv2\n",
    "import shutil\n",
    "\n",
    "#from google.colab.patches import cv2_imshow\n",
    "import os.path\n",
    "import time\n",
    "import shutil\n",
    "import glob\n",
    "import warnings\n",
    "\n",
    "def clean_folder(folder_name):\n",
    "    try:\n",
    "      shutil.rmtree(folder_name)\n",
    "    except OSError as e:\n",
    "      pass\n",
    "    os.makedirs(folder_name, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [],
   "source": [
    "yolo_path = \"detLib/\"\n",
    "sys.path.append(yolo_path)\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "from detLib.models import Darknet\n",
    "from detLib.utils.utils import *\n",
    "from detLib.utils.datasets import *\n",
    "\n",
    "def y_path(p):\n",
    "    return os.path.join(yolo_path,p)\n",
    "\n",
    "weights = y_path(\"weights/yolov3_ckpt_current_50.pth\")\n",
    "model_def = y_path(\"config/yolov3-custom.cfg\")\n",
    "model = Darknet(model_def, img_size=416).to(device)\n",
    "model.load_state_dict(torch.load(weights, map_location=device))\n",
    "model.eval()\n",
    "classes = ['pedestrian']\n",
    "conf_thres, nms_thres = 0.9, 0.4"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- 7.050244092941284 seconds for torch.Size([1, 3, 960, 960])---\n",
      "--- 0.024682998657226562 seconds for saving 27 images---\n"
     ]
    }
   ],
   "source": [
    "# une img\n",
    "\n",
    "output_dir = \"output/\"\n",
    "clean_folder(output_dir)\n",
    "\n",
    "resizeSize = 960  # if zero keep original dimnesion\n",
    "img_path = \"Frames/MOT16-10-raw/frame1.jpg\"\n",
    "\n",
    "img = cv2.imread(img_path)\n",
    "img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "img = transforms.ToTensor()(img)\n",
    "img, _ = pad_to_square(img, 0)\n",
    "\n",
    "if resizeSize != 0:\n",
    "    img = resize(img, resizeSize)\n",
    "img.unsqueeze_(0)  # add batch axis\n",
    "\n",
    "with torch.no_grad():\n",
    "    start_time = time.time()\n",
    "    detections = model(img)\n",
    "    detections = non_max_suppression(detections, conf_thres,nms_thres)\n",
    "    detections = detections[0]\n",
    "    print(f\"--- %s seconds for {img.shape}---\" % (time.time() - start_time))\n",
    "\n",
    "    start_time = time.time()\n",
    "    #img = np.array(Image.open(img_path))\n",
    "    img = cv2.imread(img_path)\n",
    "    detections = rescale_boxes(detections, resizeSize, img.shape[:2])\n",
    "    for i, (x1, y1, x2, y2, conf, cls_conf, cls_pred) in enumerate(detections):\n",
    "        box_w = x2 - x1\n",
    "        box_h = y2 - y1\n",
    "\n",
    "        if 0.45*box_h > box_w > 10:\n",
    "            pedestrian = img[int(y1):int(y2), int(x1):int(x2), :]\n",
    "            filename =f'{output_dir}/{i:04}.png'\n",
    "            cv2.imwrite(filename, pedestrian)\n",
    "    print(f\"--- %s seconds for saving {i} images---\" % (time.time() - start_time))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- 6.855797052383423 seconds for torch.Size([1, 3, 960, 960])---\n",
      "--- 0.02375197410583496 seconds for saving 27 images---\n",
      "--- 7.303478002548218 seconds for torch.Size([1, 3, 960, 960])---\n",
      "--- 0.027131080627441406 seconds for saving 18 images---\n",
      "--- 6.22790789604187 seconds for torch.Size([1, 3, 960, 960])---\n",
      "--- 0.024360179901123047 seconds for saving 27 images---\n",
      "--- 7.276079893112183 seconds for torch.Size([1, 3, 960, 960])---\n",
      "--- 0.024101972579956055 seconds for saving 31 images---\n"
     ]
    }
   ],
   "source": [
    "\n",
    "videoname = 'MOT16-10-raw'\n",
    "videoframe = cv2.VideoCapture('vids/'+videoname+'.webm')\n",
    "output_dir = \"output/\"\n",
    "clean_folder(output_dir)\n",
    "resizeSize = 960  # if zero keep original dimnesion\n",
    "\n",
    "\n",
    "framenr=0\n",
    "\n",
    "if not videoframe.isOpened() :\n",
    "  print(\"Error opening video stream or file\")\n",
    "while videoframe.isOpened():\n",
    "  # Capture frame-by-frame\n",
    "  ret, img = videoframe.read()\n",
    "  if ret:\n",
    "    framenr+=1\n",
    "\n",
    "    img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "    img = transforms.ToTensor()(img)\n",
    "    img, _ = pad_to_square(img, 0)\n",
    "\n",
    "    if resizeSize != 0:\n",
    "        img = resize(img, resizeSize)\n",
    "    img.unsqueeze_(0)  # add batch axis\n",
    "\n",
    "    with torch.no_grad():\n",
    "        start_time = time.time()\n",
    "        detections = model(img)\n",
    "        detections = non_max_suppression(detections, conf_thres,nms_thres)\n",
    "        detections = detections[0]\n",
    "        print(f\"--- %s seconds for {img.shape}---\" % (time.time() - start_time))\n",
    "\n",
    "        start_time = time.time()\n",
    "        #img = np.array(Image.open(img_path))\n",
    "        img = cv2.imread(img_path)\n",
    "        detections = rescale_boxes(detections, resizeSize, img.shape[:2])\n",
    "        for i, (x1, y1, x2, y2, conf, cls_conf, cls_pred) in enumerate(detections):\n",
    "            box_w = x2 - x1\n",
    "            box_h = y2 - y1\n",
    "\n",
    "            if 0.45*box_h > box_w > 10:\n",
    "                pedestrian = img[int(y1):int(y2), int(x1):int(x2), :]\n",
    "                filename =f'{output_dir}/{framenr}-{i:04}.png'\n",
    "                cv2.imwrite(filename, pedestrian)\n",
    "        print(f\"--- %s seconds for saving {i} images---\" % (time.time() - start_time))\n",
    "\n",
    "        if framenr > 3:\n",
    "            break\n",
    "\n",
    "videoframe.release()\n",
    "# Closes all the frames\n",
    "cv2.destroyAllWindows()\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "\"\"\"\" some test to make the model work with full size image without padding, complicate to modify cause model is for square\n",
    "from torchvision.transforms.functional import pad\n",
    "\n",
    "s = list(file.size)\n",
    "to_pad = tuple([int((31-(si-1)%32)/2) for si in s])\n",
    "print(to_pad)\n",
    "\n",
    "file = pad(file, padding=to_pad)\n",
    "print(file.size)\n",
    "\"\"\"\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}