{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 151
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 4068,
     "status": "ok",
     "timestamp": 1590843742484,
     "user": {
      "displayName": "Mirko Indumi",
      "photoUrl": "",
      "userId": "02763474582244212418"
     },
     "user_tz": -120
    },
    "id": "lmUq5kQ-A8JC",
    "outputId": "0058866e-9b4d-41de-84d6-583a4f0f4b4d"
   },
   "outputs": [],
   "source": [
    "'''\n",
    "\n",
    "#Mount drive\n",
    "\n",
    "\n",
    "class FileSystem:\n",
    "  def __init__(self, colab_dir=\"PersonTracking\", local_dir=\"./\", data_dir=\"data\"): # replace with dlav path\n",
    "    IN_COLAB = 'google.colab' in sys.modules\n",
    "    if (IN_COLAB):\n",
    "      from google.colab import drive\n",
    "      drive.mount('/gdrive')\n",
    "      self.root_dir = os.path.join(\"/gdrive/My Drive/\", colab_dir)\n",
    "    else:\n",
    "      self.root_dir = local_dir\n",
    "    self.data_dir = data_dir\n",
    "    self.change_directory = False\n",
    "\n",
    "  def data_path(self, name):\n",
    "    return os.path.join(self.data_dir, name) if self.change_directory else os.path.join(self.root_dir, self.data_dir, name)\n",
    "\n",
    "  def path(self, name):\n",
    "    return os.path.join(\"./\", name) if self.change_directory else os.path.join(\"./\", self.root_dir, name)\n",
    "\n",
    "  def cd(self):\n",
    "    %cd {self.root_dir}\n",
    "    %ls\n",
    "    self.change_directory = True\n",
    "\n",
    "fs = FileSystem()\n",
    "fs.cd()\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "K7oks2aQkZKz"
   },
   "outputs": [],
   "source": [
    "import time\n",
    "import datetime\n",
    "import argparse\n",
    "import random\n",
    "import os, sys\n",
    "\n",
    "\n",
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import datasets\n",
    "from torch.autograd import Variable\n",
    "\n",
    "import torchvision.transforms as T\n",
    "from torch.utils.data import Dataset\n",
    "from torch.utils.data import DataLoader\n",
    "from PIL import Image\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.patches as patches\n",
    "from matplotlib.ticker import NullLocator\n",
    "\n",
    "from matplotlib.colors import ListedColormap, LinearSegmentedColormap\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "from tqdm import tqdm \n",
    "\n",
    "import io\n",
    "import PIL\n",
    "from PIL import Image\n",
    "import requests\n",
    "import cv2\n",
    "import shutil\n",
    "\n",
    "#from google.colab.patches import cv2_imshow\n",
    "import os.path\n",
    "import glob\n",
    "import warnings\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 7158,
     "status": "ok",
     "timestamp": 1590843260130,
     "user": {
      "displayName": "Mirko Indumi",
      "photoUrl": "",
      "userId": "02763474582244212418"
     },
     "user_tz": -120
    },
    "id": "LqvwXczNaUws",
    "outputId": "89110da4-6e67-44f4-960b-74af0861a491"
   },
   "outputs": [],
   "source": [
    "!pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 252
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 19955,
     "status": "ok",
     "timestamp": 1590843851657,
     "user": {
      "displayName": "Mirko Indumi",
      "photoUrl": "",
      "userId": "02763474582244212418"
     },
     "user_tz": -120
    },
    "id": "d0BsRp7dCd5y",
    "outputId": "4a5fdb14-c4a7-4b42-cb05-f5023149c675"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001B[KReceiving objects:  90% (606/673), 12.88 MiB | 693.00 KiB/s\r",
      "Receiving objects:  23% (158/673), 7.29 MiB | 883.00 KiB/sReceiving objects:  38% (256/673), 10.90 MiB | 864.00 KiB/sremote: Total 673 (delta 69), reused 67 (delta 24), pack-reused 552\u001B[K\r\n",
      "\u001B[KReceiving objects: 100% (673/673), 13.29 MiB | 763.00 KiB/s, done.\r\n",
      "\u001B[KResolving deltas: 100% (409/409), done.\r\n",
      "cp: /home/fabien/Desktop/DLfAD/YOLOv3-pedestrian/weights/yolov3_ckpt_current_50.pth: No such file or directory\r\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "  shutil.rmtree('reid-strong-baseline')\n",
    "except OSError as e: \n",
    "  pass\n",
    "\n",
    "try:\n",
    "  shutil.rmtree('YOLOv3-pedestrian')\n",
    "except OSError as e: \n",
    "  pass\n",
    "\n",
    "#!git clone https://github.com/nodiz/reid-strong-baseline.git\n",
    "!git clone https://github.com/nodiz/YOLOv3-pedestrian.git\n",
    "#!cd YOLOv3-pedestrian && git checkout AnchorsForVideo\n",
    "\n",
    "!cp /home/fabien/Desktop/DLfAD/YOLOv3-pedestrian/weights/yolov3_ckpt_current_50.pth YOLOv3-pedestrian/weights"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "8DXZr9lUO412"
   },
   "source": [
    "**Affiliation**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Bzo4ZU--O27j"
   },
   "outputs": [],
   "source": [
    "def crop_affilition(distmat,dist_thld,framenr,gallery_path,name_query,name_gallery):\n",
    "\n",
    "    #For distmat: rows: Querys, Collums: Gallery\n",
    "    \n",
    "    maxiter=np.shape(distmat)[0]\n",
    "    gallength=len(os.listdir(gallery_path))\n",
    "    querynr=0\n",
    "    \n",
    "    query = np.arange(0,maxiter)\n",
    "    aff_mat=np.zeros((maxiter,2))\n",
    "\n",
    "    while querynr < maxiter:\n",
    "        \n",
    "          q_min=np.amin(distmat)\n",
    "          result = np.where(distmat == q_min)\n",
    "\n",
    "          result=(result[0][0], result[1][0])\n",
    "          idx=(result[0], result[1])\n",
    "          #print(distmat[idx], dist_thld)\n",
    "        \n",
    "\n",
    "          if (distmat[idx]<dist_thld and querynr<gallength):\n",
    "\n",
    "            #print('Put Query '+str(result[0])+' into Gallery ' +f'{result[1]}')\n",
    "\n",
    "            shutil.move(name_query[result[0]], name_gallery[result[1]][:-9]+'/'+str(framenr)+'.jpg')\n",
    "            aff_mat[querynr,:]=[result[0],int(name_gallery[result[1]][-13:-9])]\n",
    "\n",
    "            for i in range(0,np.shape(name_gallery)[0]):\n",
    "                if name_gallery[i][-13:-9] == name_gallery[result[1]][-13:-9]:\n",
    "                    distmat[:,i]=float(\"inf\")\n",
    "\n",
    "\n",
    "\n",
    "            distmat[:,result[1]]=float(\"inf\")\n",
    "            distmat[result[0],:]=float(\"inf\")\n",
    "            query = np.delete(query, np.where(query == result[0]))\n",
    "            querynr+=1\n",
    "\n",
    "          else:\n",
    "            #put remaining queries in new folder\n",
    "\n",
    "            for i in query:\n",
    "\n",
    "                #print('Put Query '+str(i)+' into new Gallery ' +f'{gallength}')\n",
    "                os.makedirs(name_gallery[0][:-14]+'/'+f'{gallength:04}', exist_ok = True)      \n",
    "                shutil.move(name_query[i], name_gallery[0][:-14]+'/'+f'{gallength:04}'+'/'+str(framenr)+'.jpg')\n",
    "                aff_mat[querynr,:]=[i,gallength]\n",
    "                gallength+=1\n",
    "                querynr+=1\n",
    "\n",
    "    return aff_mat             \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Ebhh6I_BT47i",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "**RE-ID Function**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 70
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 1562,
     "status": "ok",
     "timestamp": 1590843920108,
     "user": {
      "displayName": "Mirko Indumi",
      "photoUrl": "",
      "userId": "02763474582244212418"
     },
     "user_tz": -120
    },
    "id": "mfAdtf-HhbTH",
    "outputId": "1a1bfb14-99ed-4a3f-e5f7-abdb22492939",
    "pycharm": {
     "is_executing": true,
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# Build model\n",
    "#!ls #/gdrive#/.shortcut-targets-by-id/ #1xcANAd3HJkfxNCBAtjJOyodOgmX0iO8b/PersonTracking\n",
    "from reidLib.modeling.baseline import Baseline\n",
    "\n",
    "__file__ = 'reidLib/modeling'\n",
    "sys.path.append(os.path.dirname(__file__))\n",
    "\n",
    "def build_model(num_classes):\n",
    "    #     model = Baseline(num_classes, cfg.MODEL.LAST_STRIDE, cfg.MODEL.PRETRAIN_PATH, cfg.MODEL.NECK, cfg.TEST.NECK_FEAT \\ after)\n",
    "    model = Baseline(num_classes, 1, \"WeightsReid/resnet50-19c8e357.pth\", 'bnneck', 'after', 'resnet50', 'imageNet')   # maybe try self instead of imageNet\n",
    "    return model\n",
    "\n",
    "device = 'cuda'\n",
    "model = build_model(1041)  # 1041 identities in training dataset\n",
    "model.eval()  # evaluation mode\n",
    "#model=torch.load('WeightsReid/resnet50_ibn_a_center_param_120.pth')\n",
    "model.load_param('WeightsReid/WeightFab/center/resnet50_model_100.pth')\n",
    "model.to(device);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "PaExWreZf8t0",
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "# Prepare transform\n",
    "\n",
    "normalize_transform = T.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "\n",
    "transform = T.Compose([\n",
    "    T.Resize([256, 128]),\n",
    "    T.ToTensor(),\n",
    "    normalize_transform\n",
    "])\n",
    "\n",
    "\n",
    "#Dataset  \n",
    "\n",
    "class ImageDataset(Dataset):\n",
    "    \"\"\"Image Person ReID Dataset\"\"\"\n",
    "\n",
    "    def __init__(self, dataset, transform=None):\n",
    "        self.dataset = dataset\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.dataset)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        img_path = self.dataset[index]\n",
    "        img = Image.open(img_path).convert('RGB')\n",
    "\n",
    "        if self.transform is not None:\n",
    "            img = self.transform(img)\n",
    "\n",
    "        return img\n",
    "\n",
    "# Dataloader\n",
    "\n",
    "def val_collate_fn(batch):\n",
    "    return torch.stack(batch, dim=0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "nm86mLxyToqB",
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "def reid(gallery_path='pic/*', query_path='pic/*'):\n",
    "    \n",
    "    #Load images query \n",
    "    Gallery_size = 140\n",
    "    imgs_query = sorted(glob.glob(os.path.join(query_path, \"*.*\"))) \n",
    "    num_query = np.shape(imgs_query)[0]  # first x images in dataset\n",
    "\n",
    "    # Load images gallery\n",
    "    imgs_gallery = sorted(glob.iglob(os.path.join(gallery_path, \"*.*\")),key=os.path.getctime, reverse=True) #pic = gallery_path\n",
    "\n",
    "    if np.shape(imgs_gallery)[0]>Gallery_size :\n",
    "        imgs_gallery = imgs_gallery[:Gallery_size ]\n",
    "        \n",
    "    imgs=imgs_query + imgs_gallery\n",
    "\n",
    "    #print([os.path.basename(x) for x in imgs[:num_query]])\n",
    "    #print([os.path.basename(x) for x in imgs[num_query:]])\n",
    "\n",
    "    # build dataset and dataloader  !!!! np.shape(imgs)[0]\n",
    "    imdatas = ImageDataset(imgs, transform)\n",
    "    demo_loader = DataLoader(\n",
    "        imdatas, batch_size=np.shape(imgs)[0], shuffle=False, num_workers=4,\n",
    "        collate_fn=val_collate_fn\n",
    "    )\n",
    "\n",
    "\n",
    "    # model evaluation\n",
    "\n",
    "    with torch.no_grad():\n",
    "      for batch in demo_loader:\n",
    "        batch = batch.to(device)\n",
    "        feat = model(batch) # (bs, 2048)\n",
    "        feat_norm = 1\n",
    "        if feat_norm:\n",
    "          feat = torch.nn.functional.normalize(feat, dim=1, p=2)\n",
    "        # query\n",
    "        qf = feat[:num_query]\n",
    "\n",
    "        # gallery\n",
    "        gf = feat[num_query:]\n",
    "\n",
    "        m, n = qf.shape[0], gf.shape[0]\n",
    "\n",
    "        distmat = torch.pow(qf, 2).sum(dim=1, keepdim=True).expand(m, n) + \\\n",
    "                  torch.pow(gf, 2).sum(dim=1, keepdim=True).expand(n, m).t()\n",
    "\n",
    "        distmat.addmm_(1, -2, qf, gf.t())\n",
    "        distmat = distmat.cpu().numpy()\n",
    "\n",
    "        return distmat, imgs_query, imgs_gallery"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": true,
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def white_balance(img):\n",
    "    result = cv2.cvtColor(img, cv2.COLOR_BGR2LAB)\n",
    "    avg_a = np.average(result[:, :, 1])\n",
    "    avg_b = np.average(result[:, :, 2])\n",
    "    result[:, :, 1] = result[:, :, 1] - ((avg_a - 128) * (result[:, :, 0] / 255.0) * 1.1)\n",
    "    result[:, :, 2] = result[:, :, 2] - ((avg_b - 128) * (result[:, :, 0] / 255.0) * 1.1)\n",
    "    result = cv2.cvtColor(result, cv2.COLOR_LAB2BGR)\n",
    "    return result\n",
    "\n",
    "def show(final):\n",
    "    print('display')\n",
    "    cv2.imshow('bla', frame)\n",
    "    cv2.waitKey(0)\n",
    "    cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "PnuKSzPjs4cO",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "**Bounding Box**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "13xkcF6ns2VZ",
    "pycharm": {
     "is_executing": true,
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def bboxdrawer(img_path,txtfile_path,aff_mat, gallery_path, idx):\n",
    "  \n",
    "    # Creates bounding box by overwriting raw frame at img_path\n",
    "    aff_mat=aff_mat[aff_mat[:,0].argsort()]\n",
    "\n",
    "    img = np.array(Image.open(img_path))\n",
    "    plt.figure()\n",
    "    fig, ax = plt.subplots(1)\n",
    "    ax.imshow(img)\n",
    "  \n",
    "    cmap = plt.get_cmap(\"tab20b\")\n",
    "\n",
    "    hsvBig = plt.get_cmap('hsv', 512)\n",
    "    \n",
    "    cmap = ListedColormap(hsvBig(np.linspace(0, 1, 256)))\n",
    "  \n",
    "    colors = [cmap(i) for i in np.linspace(0, 1, 256)]\n",
    "    \n",
    "    bboxmatrix = np.loadtxt(txtfile_path)\n",
    "    \n",
    "    if np.shape(aff_mat)[0] == 1:\n",
    "        bboxmatrix[0] = aff_mat[0][1]\n",
    "        bboxmatrix=[bboxmatrix]\n",
    "        bb_boxfor=bboxmatrix\n",
    "    else:    \n",
    "        bboxmatrix[:,0]=aff_mat[:,1]\n",
    "        bboxmatrix=[bboxmatrix]\n",
    "\n",
    "        bb_boxfor=bboxmatrix[0]    \n",
    "\n",
    "    bbox_colors = np.array(colors)[idx]    \n",
    "    \n",
    "    for label,x1,x2,y1,y2 in bb_boxfor:\n",
    "\n",
    "      ''' \n",
    "      label=bboxmatrix[j,0]\n",
    "      x1=bboxmatrix[j,1]\n",
    "      x2=bboxmatrix[j,2]\n",
    "      y1=bboxmatrix[j,3]\n",
    "      y2=bboxmatrix[j,4]\n",
    "      '''\n",
    "      \n",
    "      box_w = x2 - x1\n",
    "      box_h = y2 - y1\n",
    "      color = bbox_colors[int(label-1)]\n",
    "      #print(color)\n",
    "      bbox = patches.Rectangle((x1, y1), box_w, box_h, linewidth=1, edgecolor=color, facecolor=\"none\")\n",
    "      ax.add_patch(bbox)\n",
    "      plt.text(\n",
    "                x1,\n",
    "                y1,\n",
    "                s=int(label),\n",
    "                color=\"white\",\n",
    "                verticalalignment=\"top\",\n",
    "                bbox={\"color\": color, \"pad\": 0},\n",
    "            )\n",
    "    plt.axis(\"off\")\n",
    "    plt.gca().xaxis.set_major_locator(NullLocator())\n",
    "    plt.gca().yaxis.set_major_locator(NullLocator())\n",
    "    plt.savefig(img_path, bbox_inches=\"tight\", pad_inches=0.0)\n",
    "    #plt.close()\n",
    "\n",
    "#bboxdrawer('Frames/MOT16-11-raw/frame2.jpg','DetectedImages/MOT16-11-raw/Detection2.txt')\n",
    "#bboxdrawer('Frames/MOT16-11-raw/frame3.jpg','DetectedImages/MOT16-11-raw/Detection3.txt')\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "0LudXFQgk6tu",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "**Detection Algorithm**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 30881,
     "status": "error",
     "timestamp": 1590843961126,
     "user": {
      "displayName": "Mirko Indumi",
      "photoUrl": "",
      "userId": "02763474582244212418"
     },
     "user_tz": -120
    },
    "id": "tfisITLwEo6V",
    "outputId": "c7f2962a-808d-4028-db66-5d52ba198f11",
    "pycharm": {
     "is_executing": true,
     "name": "#%%\n"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#clear sample folder in YoloV3\n",
    "\n",
    "\n",
    "files = glob.glob('YOLOv3-pedestrian/data/samples/*')\n",
    "for f in files:\n",
    "    os.remove(f)\n",
    "files = glob.glob('YOLOv3-pedestrian/output/*')\n",
    "for f in files:\n",
    "    os.remove(f)\n",
    "\n",
    "\n",
    "\n",
    "#define and read video\n",
    "\n",
    "videoname = 'MOT16-11-raw'\n",
    "threshold=0.13 #0.13\n",
    "\n",
    "\n",
    "# Color BBOX\n",
    "    \n",
    "n_cls = 150 #Maximum color \n",
    "random.seed(1)\n",
    "index=np.linspace(0,255, n_cls).astype(int)\n",
    "np.random.shuffle(index)\n",
    "\n",
    "# data aug\n",
    "hsv=True\n",
    "wbalance=True\n",
    "\n",
    "#delete at every execution folder such that code can be executed again\n",
    "try:\n",
    "  shutil.rmtree('DetectedImages/'+videoname)\n",
    "except OSError as e: \n",
    "  pass\n",
    "\n",
    "frame_path='Frames/'+videoname\n",
    "try:\n",
    "  shutil.rmtree(frame_path)\n",
    "except OSError as e: \n",
    "  pass\n",
    "os.makedirs(frame_path, exist_ok=True)\n",
    "\n",
    "videoframe = cv2.VideoCapture('VideoToTrack/'+videoname+'.webm')\n",
    "framenr=0\n",
    "firstdetection=True\n",
    "\n",
    "if (videoframe.isOpened()== False): \n",
    "  print(\"Error opening video stream or file\")\n",
    "while(videoframe.isOpened()):\n",
    "  # Capture frame-by-frame\n",
    "  ret, frame = videoframe.read()\n",
    "  if ret == True:\n",
    "    framenr+=1\n",
    "    \n",
    "    # Display the resulting frame\n",
    "    \n",
    "\n",
    "    if wbalance ==True:\n",
    "        frame = white_balance(frame)\n",
    "\n",
    "    if hsv==True:\n",
    "\n",
    "        hsvImg = cv2.cvtColor(frame,cv2.COLOR_BGR2HSV)\n",
    "\n",
    "        #multiple by a factor to change the saturation\n",
    "        hsvImg[...,1] = hsvImg[...,1]*1.5\n",
    "\n",
    "        #multiple by a factor of less than 1 to reduce the brightness \n",
    "        hsvImg[...,2] = hsvImg[...,2]*0.99\n",
    "\n",
    "        frame=cv2.cvtColor(hsvImg,cv2.COLOR_HSV2BGR)\n",
    "        \n",
    "    #cv2_imshow(frame)\n",
    "    imagename='YOLOv3-pedestrian/data/samples/'+videoname+str(framenr)+'.jpg'\n",
    "    cv2.imwrite(imagename, frame)\n",
    "    cv2.imwrite(frame_path+'/frame'+str(framenr)+'.jpg', frame)\n",
    "\n",
    "    \n",
    "    !python3 YOLOv3-pedestrian/detectbbox.py --conf_thres 0.9 --img_size 540 --image_folder YOLOv3-pedestrian/data/samples --model_def YOLOv3-pedestrian/config/yolov3-custom.cfg --output_dir YOLOv3-pedestrian/output --weights_path YOLOv3-pedestrian/weights/yolov3_ckpt_current_50.pth --class_path YOLOv3-pedestrian/data/classes.names\n",
    "    os.remove('YOLOv3-pedestrian/data/samples/'+videoname+str(framenr)+'.jpg')\n",
    "    \n",
    "    os.makedirs('DetectedImages/'+videoname, exist_ok=True)\n",
    "    os.makedirs('DetectedImages/'+videoname+'-txtfiles', exist_ok=True)\n",
    "    \n",
    "\n",
    "    #if detection is made do reidentification else put a warning that nothing was\n",
    "    if os.path.exists('YOLOv3-pedestrian/output/'+videoname+str(framenr)+'.txt') and np.size(np.loadtxt('YOLOv3-pedestrian/output/'+videoname+str(framenr)+'.txt')) != 0:\n",
    "        print(firstdetection)\n",
    "        shutil.move('YOLOv3-pedestrian/output/'+videoname+str(framenr)+'.txt', 'DetectedImages/'+videoname+'-txtfiles/Detection'+str(framenr)+'.txt')\n",
    "\n",
    "          #for the first frame put every detected person crop in separate folder\n",
    "        if firstdetection==True: \n",
    "\n",
    "            DIR = 'YOLOv3-pedestrian/output/'\n",
    "            detection = sorted(glob.glob(os.path.join(DIR, \"*.jpg\")))\n",
    "            print(detection, range(1,np.size(detection)+1) )\n",
    "            for pid1 in range(0,np.size(detection)):\n",
    "              os.mkdir('DetectedImages/'+videoname+'/'+f'{pid1:04}')\n",
    "              shutil.move('YOLOv3-pedestrian/output/'+f'{pid1+1:04}'+'.jpg', 'DetectedImages/'+videoname+'/'+f'{pid1:04}'+'/'+f'{framenr:04}'+'.jpg' )\n",
    "            \n",
    "            firstdetection=False\n",
    "\n",
    "        else: # np.size(np.loadtxt('DetectedImages/'+videoname+'-txtfiles/Detection'+str(framenr)+'.txt')) != 0:\n",
    "          #implement person reid here and affilate image to correct folder\n",
    "\n",
    "            distmat, name_query, name_gallery = reid(gallery_path = 'DetectedImages/'+videoname+'/*', query_path='YOLOv3-pedestrian/output/')\n",
    "            print('QUER', np.shape(name_query))\n",
    "            print('GAL', np.shape(name_gallery))\n",
    "            amat=crop_affilition(distmat= distmat, dist_thld = threshold, framenr=f'{framenr:04}', gallery_path = 'DetectedImages/'+videoname, name_query = name_query, name_gallery = name_gallery) \n",
    "            bboxdrawer(frame_path+'/frame'+str(framenr)+'.jpg','DetectedImages/'+videoname+'-txtfiles/'+'Detection'+str(framenr)+'.txt',amat,gallery_path = 'DetectedImages/'+videoname, idx=index)      \n",
    "            \n",
    "            \n",
    "            \n",
    "            torch.cuda.empty_cache()\n",
    "    elif os.path.exists('YOLOv3-pedestrian/output/'+videoname+str(framenr)+'.txt') and np.size(np.loadtxt('YOLOv3-pedestrian/output/'+videoname+str(framenr)+'.txt')) == 0: #pippo\n",
    "        os.remove('YOLOv3-pedestrian/output/'+videoname+str(framenr)+'.txt')\n",
    "        warnings.warn('no detection in frame number '+str(framenr))\n",
    "    else:\n",
    "        warnings.warn('no detection in frame number '+str(framenr))\n",
    "    \n",
    "  # Break the loop when video is finished\n",
    "  else: #here\n",
    "    break\n",
    "\n",
    "# When everything done, release the video capture object\n",
    "\n",
    "videoframe.release()\n",
    "# Closes all the frames\n",
    "cv2.destroyAllWindows()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": true,
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "np.shape('YOLOv3-pedestrian/output/'+videoname+str(framenr)+'.txt') != ()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "-ywIWNlNgU6U",
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "!pwd\n",
    "!ls DetectedImages/MOT16-10-raw/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "VEqH1TLh2ocW",
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "   '''\n",
    "    \n",
    "    aff_mat=amat\n",
    "    \n",
    "    aff_mat=aff_mat[aff_mat[:,0].argsort()]\n",
    "\n",
    "    cmap = plt.get_cmap(\"tab20b\")\n",
    "\n",
    "    hsvBig = plt.get_cmap('hsv', 512)\n",
    "    \n",
    "    cmap = ListedColormap(hsvBig(np.linspace(0, 1, 256)))\n",
    "  \n",
    "    colors = [cmap(i) for i in np.linspace(0, 1, 256)]\n",
    "    \n",
    "    bboxmatrix = np.loadtxt('DetectedImages/'+videoname+'-txtfiles/'+'Detection'+str(framenr)+'.txt')\n",
    "    \n",
    "    if np.shape(aff_mat)[0] == 1:\n",
    "        bboxmatrix[0] = aff_mat[0][1]\n",
    "        bboxmatrix=[bboxmatrix]\n",
    "        bb_boxfor=bboxmatrix\n",
    "    else:    \n",
    "        bboxmatrix[:,0]=aff_mat[:,1]\n",
    "        bb_boxfor=bboxmatrix[0]\n",
    "   \n",
    "    #if type(bboxmatrix)==np.ndarray:\n",
    "      #print('pippo')\n",
    "      #bboxmatrix=[bboxmatrix]\n",
    "\n",
    "\n",
    "    n_cls=len(os.listdir('DetectedImages/'+videoname))\n",
    "    \n",
    "    \n",
    "    random.seed(1)\n",
    "    bbox_colors = random.sample(colors, n_cls)\n",
    "    \n",
    "    print(bb_boxfor)\n",
    "    \n",
    "    for label,x1,x2,y1,y2 in bb_boxfor:\n",
    "        print('pippo')\n",
    "        \n",
    "    '''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "TxB5R3a2tPEa",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Frame to video"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "QxUPafD6C-Vo",
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "image_folder = 'Frames/'+videoname +'/'\n",
    "video_name = 'Video/'+videoname+'_BoundingBoxv3.avi'\n",
    "\n",
    "#images = [img for img in os.listdir(image_folder) if img.endswith(\".jpg\")]\n",
    "#print(images[0:10])\n",
    "images = sorted(glob.iglob(os.path.join(image_folder, \"*.jpg\")),key=os.path.getctime, reverse=True)[::-1]\n",
    "images= images[1:]\n",
    "\n",
    "frame = cv2.imread(images[0])\n",
    "height, width, layers = frame.shape\n",
    "\n",
    "video = cv2.VideoWriter(video_name, 0, 5, (width,height))\n",
    "\n",
    "print(height,width)\n",
    "\n",
    "\n",
    "for image in images:\n",
    "    a = cv2.imread(image)\n",
    "    a = cv2.resize(a, (width,height), interpolation = cv2.INTER_AREA)\n",
    "    print(image, a.shape)\n",
    "    video.write(a)\n",
    "\n",
    "cv2.destroyAllWindows()\n",
    "video.release()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "mcqMC0W7DhYh",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Tests (not important)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": true,
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "''''\n",
    "\n",
    "videoname = 'MOT16-10-raw'\n",
    "videoframe = cv2.VideoCapture('VideoToTrack/'+videoname+'.webm')\n",
    "\n",
    "framenr=0\n",
    "while(videoframe.isOpened()):\n",
    "  ret, frame = videoframe.read()\n",
    "  if ret ==True:\n",
    "  #cv2_imshow(frame)\n",
    "    framenr+=1\n",
    "    imagename='YOLOv3-pedestrian/data/samples/'+videoname+str(framenr)+'.jpg'\n",
    "    cv2.imwrite(imagename, frame)\n",
    "\n",
    "    %cd YOLOv3-pedestrian/  \n",
    "    !python detect.py --model_def config/yolov3-custom.cfg --weights_path weights/yolov3_ckpt_current_50.pth\n",
    "    os.remove('data/samples/'+videoname+str(framenr)+'.jpg')\n",
    "    %cd ..\n",
    "    os.makedirs('Detection/'+videoname, exist_ok=True)\n",
    "    shutil.move('YOLOv3-pedestrian/output/'+videoname+str(framenr)+'.png', 'Detection/'+videoname+str(framenr)+'.png' )\n",
    "    #cv2_imshow( 'Detection/'+videoname+str(framenr)+'.png')\n",
    "  \n",
    "  else:\n",
    "    break\n",
    "    \n",
    "\n",
    "videoframe.release()\n",
    "# Closes all the frames\n",
    "cv2.destroyAllWindows()\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": true,
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "videoname = 'MOT16-11-raw'\n",
    "videoframe = cv2.VideoCapture('VideoToTrack/'+videoname+'.webm')\n",
    "\n",
    "\n",
    "if (videoframe.isOpened()== False): \n",
    "  print(\"Error opening video stream or file\")\n",
    "else:\n",
    "  ret, frame = videoframe.read()\n",
    "  cv2.imshow('blabla.jpg',frame)\n",
    "\n",
    "\n",
    "\n",
    "hsvImg = cv2.cvtColor(frame,cv2.COLOR_BGR2HSV)\n",
    "\n",
    "#multiple by a factor to change the saturation\n",
    "hsvImg[...,1] = hsvImg[...,1]*1.5\n",
    "\n",
    "#multiple by a factor of less than 1 to reduce the brightness \n",
    "hsvImg[...,2] = hsvImg[...,2]*0.5\n",
    "\n",
    "image=cv2.cvtColor(hsvImg,cv2.COLOR_HSV2BGR)\n",
    "\n",
    "cv2.imshow('blabla2.jpg',image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": true,
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "!pwd\n",
    "!ls\n",
    "#os.remove('MOT16-10-raw1.jpg')\n",
    "!pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "BXh7JcvaDgxu",
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "!pwd\n",
    "!ls\n",
    "#os.remove('MOT16-10-raw1.jpg')\n",
    "!pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "_cxPFC-AMQH5",
    "pycharm": {
     "is_executing": true,
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "videoname = 'MOT16-11-raw'\n",
    "videoframe = cv2.VideoCapture('VideoToTrack/'+videoname+'.webm')\n",
    "\n",
    "\n",
    "if (videoframe.isOpened()== False): \n",
    "  print(\"Error opening video stream or file\")\n",
    "else:\n",
    "  ret, frame = videoframe.read()\n",
    "  cv2.imshow('blabla.jpg',frame)\n",
    "\n",
    "\n",
    "\n",
    "hsvImg = cv2.cvtColor(frame,cv2.COLOR_BGR2HSV)\n",
    "\n",
    "#multiple by a factor to change the saturation\n",
    "hsvImg[...,1] = hsvImg[...,1]*1.5\n",
    "\n",
    "#multiple by a factor of less than 1 to reduce the brightness \n",
    "hsvImg[...,2] = hsvImg[...,2]*0.5\n",
    "\n",
    "image=cv2.cvtColor(hsvImg,cv2.COLOR_HSV2BGR)\n",
    "\n",
    "cv2.imshow('blabla2.jpg',image)"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "VideoPersonTracking.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}